import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import timedelta
import io

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    accuracy_score,
    confusion_matrix,
    ConfusionMatrixDisplay
)

from modules.utils.data_master import construir_tabla_maestra
import plotly.graph_objects as go

if "dataset_ampliado" not in st.session_state:
    st.session_state["dataset_ampliado"] = None


def mostrar_ampliacion_dataset(datasets):
    """Vista interactiva mejorada para ampliar el dataset con mayor variabilidad, nuevos clientes y ventas m√°s realistas."""
    st.subheader("üß© Ampliaci√≥n Avanzada del Dataset con Variabilidad Realista")

    # 1Ô∏è‚É£ Construir la tabla maestra base
    df = construir_tabla_maestra(datasets, mostrar_mensajes=False)
    if df.empty:
        st.warning("‚ö†Ô∏è No hay datos disponibles para ampliar.")
        return

    if "id_venta" not in df.columns or "cantidad" not in df.columns:
        st.error("‚ùå La tabla maestra debe tener las columnas 'id_venta' y 'cantidad'.")
        return

    if "fecha" in df.columns:
        df["fecha"] = pd.to_datetime(df["fecha"], errors="coerce")

    base_ventas = df["id_venta"].nunique()
    base_productos = df.get("nombre_producto", pd.Series()).nunique()
    base_categorias = df.get("categoria", pd.Series()).nunique()
    base_cantidades = df["cantidad"].sum()

    # 2Ô∏è‚É£ Factor de ampliaci√≥n
    factor = st.slider("Multiplicar dataset por:", 1, 20, 1)
    st.markdown("Aumenta el tama√±o del dataset simulando nuevas ventas con variaciones m√°s amplias y realistas.")

    # 3Ô∏è‚É£ Generar dataset ampliado con variabilidad
    df_extendido = []
    for i in range(factor):
        df_copy = df.copy()

        # Regenerar IDs √∫nicos
        df_copy["id_venta"] = df_copy["id_venta"].astype(str) + f"_{i+1}"

        # Desplazar fechas (simulaci√≥n estacional)
        if "fecha" in df_copy.columns:
            df_copy["fecha"] = df_copy["fecha"] + timedelta(days=np.random.randint(10, 60) * i)

        # Variar importes con ruido normal (¬±35%)
        if "importe_total" in df_copy.columns:
            variacion_importe = np.random.normal(1.0, 0.35, len(df_copy))
            df_copy["importe_total"] = (df_copy["importe_total"] * variacion_importe).clip(lower=50).round(2)

        # üîπ NUEVO: generar variabilidad avanzada en cantidad
        # Se basa en importe_total + ruido aleatorio + categor√≠a
        base_cantidad = df_copy["cantidad"] * np.random.uniform(0.5, 2.0, len(df_copy))
        ruido = np.random.normal(1.0, 0.4, len(df_copy))  # m√°s dispersi√≥n
        df_copy["cantidad"] = (base_cantidad * ruido).round().astype(int)
        df_copy["cantidad"] = df_copy["cantidad"].clip(lower=1, upper=50)  # ampliar rango m√°ximo

        # Simular nuevos clientes
        if "cliente" in df_copy.columns:
            nuevos_clientes = [f"Cliente_{np.random.randint(1000, 9999)}" for _ in range(len(df_copy))]
            df_copy["cliente"] = np.where(np.random.rand(len(df_copy)) < 0.4, nuevos_clientes, df_copy["cliente"])

        # Variar categor√≠as y productos
        if "categoria" in df_copy.columns:
            df_copy["categoria"] = df_copy["categoria"].apply(
                lambda c: c if np.random.rand() > 0.2 else f"{c}_Alt{i+1}"
            )

        if "nombre_producto" in df_copy.columns:
            df_copy["nombre_producto"] = df_copy["nombre_producto"].apply(
                lambda p: p if np.random.rand() > 0.1 else f"{p}_V{i+1}"
            )

        df_extendido.append(df_copy)

    df_extendido = pd.concat(df_extendido, ignore_index=True)

    # üîπ Extra: generar comportamiento no lineal (precio bajo ‚Üí m√°s cantidad)
    if "importe_total" in df_extendido.columns:
        correlador = np.random.uniform(0.5, 1.5, len(df_extendido))
        df_extendido["cantidad"] = (
            (df_extendido["cantidad"] * (1 / np.log1p(df_extendido["importe_total"])) * 20 * correlador)
            .round().astype(int)
        )
        df_extendido["cantidad"] = df_extendido["cantidad"].clip(lower=1, upper=60)

    # 4Ô∏è‚É£ Calcular m√©tricas
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("üßæ Ventas √∫nicas", f"{df_extendido['id_venta'].nunique():,}", f"x{factor}")
    col2.metric("üì¶ Productos", f"{df_extendido['nombre_producto'].nunique():,}")
    col3.metric("üè∑Ô∏è Categor√≠as", f"{df_extendido['categoria'].nunique():,}")
    col4.metric("üìä Total de cantidades", f"{df_extendido['cantidad'].sum():,}")

    # 5Ô∏è‚É£ Visualizaci√≥n de dispersi√≥n
    fig = px.scatter(
        df_extendido.sample(min(1000, len(df_extendido))),
        x="importe_total",
        y="cantidad",
        color="categoria" if "categoria" in df_extendido.columns else None,
        title="üìâ Dispersi√≥n de Importe Total vs Cantidad Vendida",
        labels={"importe_total": "Importe Total ($)", "cantidad": "Cantidad Vendida (unidades)"},
        opacity=0.6
    )
    fig.update_traces(marker=dict(size=6))
    fig.update_layout(template="plotly_white")
    st.plotly_chart(fig, use_container_width=True)

    # 6Ô∏è‚É£ Guardar en memoria
    st.session_state["dataset_ampliado"] = df_extendido
    st.success(f"‚úÖ Dataset ampliado ({len(df_extendido)} registros) guardado en memoria global.")
    st.info("üí° Ahora las cantidades tienen mayor dispersi√≥n y relaci√≥n no lineal con los importes, ideal para entrenamiento ML.")

    # 7Ô∏è‚É£ Vista previa
    st.markdown("### üìÑ Vista previa del dataset ampliado")
    st.dataframe(df_extendido.sample(10), use_container_width=True)

    # 8Ô∏è‚É£ Exportaci√≥n
    st.markdown("### üíæ Exportar dataset ampliado")
    buffer_csv = io.BytesIO()
    buffer_excel = io.BytesIO()
    df_extendido.to_csv(buffer_csv, index=False)
    df_extendido.to_excel(buffer_excel, index=False, sheet_name="dataset_ampliado")

    col_a, col_b = st.columns(2)
    col_a.download_button(
        label="üì• Descargar como CSV",
        data=buffer_csv.getvalue(),
        file_name="dataset_ampliado.csv",
        mime="text/csv"
    )
    col_b.download_button(
        label="üìò Descargar como Excel",
        data=buffer_excel.getvalue(),
        file_name="dataset_ampliado.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    if st.button("üßπ Limpiar dataset ampliado de memoria"):
        st.session_state["dataset_ampliado"] = None
        st.info("‚úÖ Dataset ampliado eliminado de memoria.")

    return df_extendido




# ============================================================
# 1Ô∏è‚É£ PREPARACI√ìN DE DATOS
# ============================================================

def preparar_datos_regresion(datasets):
    """Construye la tabla maestra de Aurelion y genera datos base para ML."""
    df = construir_tabla_maestra(datasets, mostrar_mensajes=False)
    if df.empty:
        st.warning("‚ö†Ô∏è No hay datos para an√°lisis predictivo.")
        return pd.DataFrame()

    df["a√±o"] = pd.to_datetime(df["fecha"], errors="coerce").dt.year
    df["mes"] = pd.to_datetime(df["fecha"], errors="coerce").dt.month

    df = df[["nombre_producto", "categoria", "cantidad", "importe_total", "mes", "a√±o"]]
    return df


# ============================================================
# 2Ô∏è‚É£ GENERAR DATOS SINT√âTICOS (AMPLIAR DATASET)
# ============================================================

def ampliar_dataset(df):
    """Permite aumentar el tama√±o del dataset para entrenamiento."""
    st.sidebar.subheader("üß© Ampliar dataset de entrenamiento")
    factor = st.sidebar.slider("Multiplicar dataset por:", 1, 10, 1)
    df_extended = pd.concat([df] * factor, ignore_index=True)
    st.info(f"‚úÖ Dataset ampliado a {len(df_extended)} registros (x{factor})")
    return df_extended


# ============================================================
# 3Ô∏è‚É£ REGRESI√ìN: LINEAL vs KNN vs RFR
# ============================================================

def comparar_regresiones(df):
    """Compara modelos de regresi√≥n (Linear, KNN y RandomForest) e interpreta resultados."""
    st.subheader("üìà Comparaci√≥n de Modelos de Regresi√≥n")
    st.markdown("Modelos incluidos: **Linear Regression**, **KNeighbors Regressor** y **Random Forest Regressor**.")
    st.write("---")

    # ============================================================
    # 1Ô∏è‚É£ Preparaci√≥n de los datos
    # ============================================================
    X = df[["importe_total", "mes"]]
    y = df["cantidad"]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )

    # ============================================================
    # 2Ô∏è‚É£ Entrenamiento de modelos
    # ============================================================
    lr = LinearRegression()
    knn = KNeighborsRegressor(n_neighbors=5)
    rf = RandomForestRegressor(n_estimators=100, random_state=42)

    lr.fit(X_train, y_train)
    knn.fit(X_train, y_train)
    rf.fit(X_train, y_train)

    y_pred_lr = lr.predict(X_test)
    y_pred_knn = knn.predict(X_test)
    y_pred_rf = rf.predict(X_test)

    # ============================================================
    # 3Ô∏è‚É£ M√©tricas comparativas
    # ============================================================
    resultados = pd.DataFrame({
        "Modelo": ["Linear Regression", "KNN Regressor", "Random Forest"],
        "MSE": [
            mean_squared_error(y_test, y_pred_lr),
            mean_squared_error(y_test, y_pred_knn),
            mean_squared_error(y_test, y_pred_rf)
        ],
        "R¬≤": [
            r2_score(y_test, y_pred_lr),
            r2_score(y_test, y_pred_knn),
            r2_score(y_test, y_pred_rf)
        ]
    }).sort_values(by="R¬≤", ascending=False)

    mejor_modelo = resultados.iloc[0]
    st.markdown("### üìã M√©tricas de Evaluaci√≥n")
    st.dataframe(resultados.style.format({"MSE": "{:.2f}", "R¬≤": "{:.3f}"}))

    # --- üîç Explicaci√≥n de las m√©tricas ---
    with st.expander("‚ÑπÔ∏è ¬øQu√© significan estas m√©tricas?"):
        st.markdown("""
        - **MSE (Error Cuadr√°tico Medio)**: mide cu√°nto se desv√≠a la predicci√≥n del valor real.  
          üîπ Cuanto **menor** sea el MSE, **mejor precisi√≥n** del modelo.  
          üîπ Si el MSE es alto, el modelo tiene mayor error en sus estimaciones.
        
        - **R¬≤ (Coeficiente de Determinaci√≥n)**: mide qu√© tan bien el modelo explica la variabilidad de los datos.  
          üîπ Valores cercanos a **1.0** indican una predicci√≥n muy precisa.  
          üîπ Valores cercanos a **0.0** indican que el modelo no logra explicar bien las variaciones.
        """)

    st.success(f"üèÜ Mejor modelo: **{mejor_modelo['Modelo']}** con R¬≤ = {mejor_modelo['R¬≤']:.3f}")

    # ============================================================
    # 4Ô∏è‚É£ Gr√°fico comparativo de curvas de predicci√≥n
    # ============================================================
    X_grid = np.linspace(X["importe_total"].min(), X["importe_total"].max(), 200).reshape(-1, 1)
    X_grid_full = np.hstack((X_grid, np.full_like(X_grid, X["mes"].mean())))

    y_grid_lr = lr.predict(X_grid_full)
    y_grid_knn = knn.predict(X_grid_full)
    y_grid_rf = rf.predict(X_grid_full)

    fig = go.Figure()

    fig.add_trace(go.Scatter(x=X_train["importe_total"], y=y_train, mode="markers",
                             name="Train", opacity=0.5, marker=dict(color="gray")))
    fig.add_trace(go.Scatter(x=X_test["importe_total"], y=y_test, mode="markers",
                             name="Test", marker=dict(color="black", symbol="x")))

    fig.add_trace(go.Scatter(x=X_grid.flatten(), y=y_grid_lr, mode="lines",
                             name="Linear Regression", line=dict(width=3, color="#1f77b4")))
    fig.add_trace(go.Scatter(x=X_grid.flatten(), y=y_grid_knn, mode="lines",
                             name="KNN Regressor", line=dict(width=3, color="#2ca02c")))
    fig.add_trace(go.Scatter(x=X_grid.flatten(), y=y_grid_rf, mode="lines",
                             name="Random Forest", line=dict(width=3, color="#d62728")))

    fig.update_layout(
        title="üìâ Comparaci√≥n de Modelos de Regresi√≥n",
        xaxis_title="Importe Total ($)",
        yaxis_title="Cantidad Vendida (unidades)",
        legend_title="Modelos",
        template="plotly_white"
    )
    st.plotly_chart(fig, use_container_width=True)

    # --- Texto explicativo del gr√°fico ---
    st.markdown("""
    **üß† Interpretaci√≥n:**
    - Cada l√≠nea representa c√≥mo el modelo predice la cantidad vendida en funci√≥n del importe total.
    - Las l√≠neas **verdes y rojas** (KNN y Random Forest) capturan mejor las variaciones no lineales, mientras que la **azul** (Linear Regression) asume una relaci√≥n m√°s r√≠gida.
    - Las l√≠neas horizontales o escalonadas pueden deberse a una cantidad limitada de datos o valores repetidos en los conjuntos de prueba.
    
    üí° *Recomendaci√≥n:* ampliar el dataset de entrenamiento con m√°s variabilidad en las ventas o aplicar una normalizaci√≥n previa ayudar√° a suavizar las predicciones.
    """)

    # ============================================================
    # 5Ô∏è‚É£ Gr√°fico resumen de rendimiento
    # ============================================================
    fig_bar = px.bar(
        resultados,
        x="Modelo",
        y="R¬≤",
        color="Modelo",
        text=resultados["R¬≤"].apply(lambda x: f"{x:.3f}"),
        title="üîç Comparaci√≥n de rendimiento (R¬≤ por modelo)",
        color_discrete_sequence=["#1f77b4", "#2ca02c", "#d62728"]
    )
    fig_bar.update_traces(textposition="outside")
    fig_bar.update_layout(yaxis_title="R¬≤ Score", xaxis_title=None)
    st.plotly_chart(fig_bar, use_container_width=True)

    # --- Conclusi√≥n gerencial ---
    st.markdown("""
    ### üèÅ Conclusi√≥n Gerencial
    - **Random Forest** muestra el mejor desempe√±o (mayor R¬≤), lo que significa que logra explicar mejor las variaciones en las ventas.  
    - **Linear Regression** tiene una precisi√≥n moderada, √∫til para entender relaciones simples entre variables.  
    - **KNN Regressor** captura patrones locales, pero puede verse afectado por la dispersi√≥n o poca densidad de datos.
    
    üî∏ **Para Aurelion**, esto significa:
    - El modelo Random Forest es el m√°s confiable para predecir las cantidades vendidas por producto o categor√≠a.
    - Puede emplearse para **planificaci√≥n de inventario, ajustes de precios y proyecci√≥n de demanda.**
    - Los resultados de R¬≤ indican que todav√≠a hay espacio para mejora (ideal > 0.8), lo cual se puede lograr **aumentando el dataset** o **incorporando nuevas variables predictoras** (como ciudad, cliente o medio de pago).
    """)

    # ============================================================
    # 6Ô∏è‚É£ Predicci√≥n de ventas futuras (simulaci√≥n interactiva)
    # ============================================================
    st.write("---")
    st.markdown("### üîÆ Predicci√≥n de Ventas Futuras (Simulaci√≥n)")
    st.markdown("""
    Usa el modelo de regresi√≥n entrenado para **predecir cu√°ntas unidades se vender√°n** seg√∫n un importe total estimado y un mes.  
    Puedes elegir el modelo que prefieras para comparar c√≥mo var√≠an los resultados.
    """)

    # --- Selecci√≥n del modelo para predecir ---
    modelo_seleccionado = st.selectbox(
        "üìò Selecciona el modelo a utilizar para la predicci√≥n:",
        ["Linear Regression", "KNeighbors Regressor", "Random Forest Regressor"],
        index=2  # por defecto Random Forest
    )

    # Entradas del usuario
    col1, col2 = st.columns(2)
    importe_usuario = col1.number_input(
        "üíµ Importe total estimado ($)", min_value=100.0, max_value=30000.0, value=5000.0, step=100.0
    )
    mes_usuario = col2.slider("üìÜ Mes proyectado", 1, 12, 6)

    # Seleccionar modelo
    if modelo_seleccionado == "Linear Regression":
        modelo = lr
    elif modelo_seleccionado == "KNeighbors Regressor":
        modelo = knn
    else:
        modelo = rf

    # Realizar predicci√≥n
    X_nueva = pd.DataFrame([[importe_usuario, mes_usuario]], columns=["importe_total", "mes"])
    prediccion = modelo.predict(X_nueva)[0]

    # Mostrar resultado
    st.metric("üì¶ Cantidad estimada a vender (unidades)", f"{prediccion:.2f}", help="Predicci√≥n generada por el modelo seleccionado.")

    # ============================================================
    # Gr√°fico interactivo de simulaci√≥n
    # ============================================================
    fig_pred = go.Figure()
    fig_pred.add_trace(go.Scatter(
        x=X["importe_total"], y=y,
        mode="markers", name="Datos hist√≥ricos", opacity=0.5, marker=dict(color="gray")
    ))
    fig_pred.add_trace(go.Scatter(
        x=[importe_usuario], y=[prediccion],
        mode="markers+text", name="Predicci√≥n futura",
        text=[f"{prediccion:.2f} unidades"],
        textposition="top center",
        marker=dict(size=12, color="red", symbol="star")
    ))

    fig_pred.update_layout(
        title=f"üìä Predicci√≥n simulada con {modelo_seleccionado}",
        xaxis_title="Importe Total ($)",
        yaxis_title="Cantidad Vendida (unidades)",
        template="plotly_white"
    )
    st.plotly_chart(fig_pred, use_container_width=True)

    # ============================================================
    # Interpretaci√≥n contextual seg√∫n modelo
    # ============================================================
    if modelo_seleccionado == "Linear Regression":
        st.markdown(f"""
        **üß† Interpretaci√≥n con Linear Regression:**
        - El modelo asume una **relaci√≥n lineal** entre el importe total y las unidades vendidas.
        - Tiende a generalizar bien cuando las ventas crecen de forma constante con el importe.
        - Para un importe de **${importe_usuario:,.0f}** en el mes **{mes_usuario}**, predice unas **{prediccion:.1f} unidades**.
        - Puede no capturar comportamientos at√≠picos o estacionales, pero es √∫til para una **visi√≥n global y tendencia general**.
        """)
    elif modelo_seleccionado == "KNeighbors Regressor":
        st.markdown(f"""
        **üß† Interpretaci√≥n con KNN Regressor:**
        - Este modelo se basa en **vecinos m√°s cercanos** para estimar la cantidad vendida.
        - Captura patrones locales, pero su precisi√≥n depende de la densidad y variedad de los datos.
        - En este escenario (importe ${importe_usuario:,.0f}, mes {mes_usuario}), estima **{prediccion:.1f} unidades**.
        - Puede verse afectado si los datos hist√≥ricos est√°n agrupados o hay pocos puntos de referencia.
        """)
    else:
        st.markdown(f"""
        **üß† Interpretaci√≥n con Random Forest Regressor:**
        - Modelo basado en **m√∫ltiples √°rboles de decisi√≥n** que combinan resultados para mejorar la predicci√≥n.
        - Es el m√°s robusto ante fluctuaciones y no linealidades en los datos.
        - Predice que para un importe de **${importe_usuario:,.0f}** en el mes **{mes_usuario}**, se vender√°n aproximadamente **{prediccion:.1f} unidades**.
        - Ideal para escenarios reales con variaciones de demanda, promociones o estacionalidad.
        """)

    # Pie explicativo
    st.markdown("""
    ---
    üßæ **Nota de interpretaci√≥n:**  
    - El eje **Y (Cantidad Vendida)** est√° expresado en **unidades de producto vendidas**.  
    - Si las predicciones parecen discretas o ‚Äúescalonadas‚Äù, se debe a la escala del dataset original.  
    - Puedes mejorar la precisi√≥n aumentando el dataset o incorporando nuevas variables predictoras (por ejemplo, ciudad o medio de pago).
    """)


    return lr, knn, rf, X_test, y_test, y_pred_lr, y_pred_knn, y_pred_rf




# ============================================================
# 4Ô∏è‚É£ CLASIFICACI√ìN Y MATRIZ DE CONFUSI√ìN
# ============================================================

def mostrar_matriz_confusion(df):
    """Entrena un modelo de clasificaci√≥n (baja rotaci√≥n) y muestra matriz de confusi√≥n con interpretaci√≥n gerencial."""
    st.subheader("üîç An√°lisis de Clasificaci√≥n: Matriz de Confusi√≥n e Interpretaci√≥n Gerencial")

    # ================================
    # 1Ô∏è‚É£ Preparaci√≥n de los datos
    # ================================
    if df.empty or "cantidad" not in df.columns or "importe_total" not in df.columns:
        st.warning("‚ö†Ô∏è No hay suficientes datos o columnas ('cantidad', 'importe_total') para realizar la clasificaci√≥n.")
        return

    # Crear columna binaria (0 = rotaci√≥n normal, 1 = baja rotaci√≥n)
    threshold = df["cantidad"].median()
    df["baja_rotacion"] = (df["cantidad"] < threshold).astype(int)

    X = df[["importe_total", "mes"]] if "mes" in df.columns else df[["importe_total"]]
    y = df["baja_rotacion"]

    # Divisi√≥n del dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # ================================
    # 2Ô∏è‚É£ Entrenamiento del modelo
    # ================================
    modelo = LogisticRegression(max_iter=200)
    modelo.fit(X_train, y_train)
    y_pred = modelo.predict(X_test)

    # ================================
    # 3Ô∏è‚É£ M√©tricas principales
    # ================================
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    st.metric("üéØ Precisi√≥n del modelo", f"{acc*100:.2f}%")

    fig, ax = plt.subplots(figsize=(6, 5))
    disp = ConfusionMatrixDisplay(cm)
    disp.plot(cmap="viridis", ax=ax, colorbar=True)
    plt.title("Matriz de Confusi√≥n - Baja Rotaci√≥n", fontsize=13)
    plt.xlabel("Etiqueta Predicha")
    plt.ylabel("Etiqueta Verdadera")
    st.pyplot(fig)

    # ================================
    # 4Ô∏è‚É£ Interpretaci√≥n de resultados
    # ================================
    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)
    total = tn + fp + fn + tp
    precision = (tp + tn) / total if total > 0 else 0

    st.markdown("### üß† Interpretaci√≥n del Modelo de Clasificaci√≥n")
    st.markdown(f"""
    El modelo predice si un producto tiene **baja rotaci√≥n (1)** o **rotaci√≥n normal (0)**.

    - **Verdaderos positivos (TP = {tp})** ‚Üí productos correctamente identificados como de baja rotaci√≥n.  
    - **Falsos positivos (FP = {fp})** ‚Üí productos mal clasificados como baja rotaci√≥n (error tipo I).  
    - **Falsos negativos (FN = {fn})** ‚Üí productos que eran de baja rotaci√≥n pero el modelo no los detect√≥ (error tipo II).  
    - **Verdaderos negativos (TN = {tn})** ‚Üí productos correctamente clasificados como de rotaci√≥n normal.  
    """)

    st.info(f"üìä La precisi√≥n total del modelo es del **{precision*100:.2f}%**, lo que significa que predice correctamente aproximadamente {precision*100:.1f}% de los casos analizados.")

    # ================================
    # 5Ô∏è‚É£ Conclusiones gerenciales
    # ================================
    st.markdown("### üí¨ Conclusiones Gerenciales")

    interpretaciones = []
    if tp > fn:
        interpretaciones.append("‚úÖ El modelo es bueno identificando productos con baja rotaci√≥n, lo que permite priorizar promociones o estrategias para esos art√≠culos.")
    else:
        interpretaciones.append("‚ö†Ô∏è El modelo tiene dificultad para detectar productos de baja rotaci√≥n. Ser√≠a recomendable aumentar la variabilidad de datos o incluir m√°s variables como categor√≠a, temporada o cliente.")
    
    if fp > 0.3 * total:
        interpretaciones.append("‚ö†Ô∏è Existen varios falsos positivos: el modelo podr√≠a estar se√±alando productos normales como 'baja rotaci√≥n', lo que podr√≠a llevar a promociones innecesarias.")
    
    if acc > 0.8:
        interpretaciones.append("üí™ El modelo muestra un buen nivel de precisi√≥n, adecuado para apoyar decisiones comerciales de reposici√≥n o liquidaci√≥n de stock.")
    elif acc > 0.6:
        interpretaciones.append("üü† El modelo tiene una precisi√≥n moderada. Puede servir como referencia inicial, pero conviene optimizarlo con m√°s variables o t√©cnicas avanzadas.")
    else:
        interpretaciones.append("üî¥ El modelo a√∫n no tiene precisi√≥n suficiente para una toma de decisiones confiable. Se recomienda ajustar la proporci√≥n de clases o ampliar el dataset.")

    for i in interpretaciones:
        st.markdown(f"- {i}")

    # ================================
    # 6Ô∏è‚É£ Recomendaciones para el negocio
    # ================================
    st.markdown("### üí° Recomendaciones para Aurelion")

    st.markdown("""
    - **Productos identificados con baja rotaci√≥n (1)**: planificar estrategias de promoci√≥n, combos o descuentos para acelerar su salida.
    - **Productos de rotaci√≥n normal (0)**: mantener niveles de inventario estables.
    - **Falsos negativos** (productos lentos no detectados) pueden generar sobrestock; se recomienda incluir m√°s variables (por ejemplo, categor√≠a o frecuencia de venta).
    - Incorporar en el futuro variables como: tipo de cliente, canal de venta, regi√≥n o temporada, para mejorar la capacidad predictiva.
    - Evaluar modelos no lineales (por ejemplo, RandomForestClassifier) para capturar relaciones m√°s complejas.
    """)

    return modelo, X_test, y_test, y_pred



# ============================================================
# 5Ô∏è‚É£ FUNCI√ìN PRINCIPAL
# ============================================================

def mostrar_prediccion_ventas_view(datasets):
    """Vista principal del m√≥dulo de predicci√≥n de ventas y clasificaci√≥n."""
    st.title("üìä Predicci√≥n y Clasificaci√≥n de Ventas - Aurelion")

    # Submen√∫ interno
    submenu = st.radio(
        "Selecciona una secci√≥n:",
        ["Ampliaci√≥n del dataset", "Predicci√≥n (Regresi√≥n)", "Clasificaci√≥n (Matriz de Confusi√≥n)"],
        horizontal=True
    )

    if submenu == "Ampliaci√≥n del dataset":
        mostrar_ampliacion_dataset(datasets)
    elif submenu == "Predicci√≥n (Regresi√≥n)":
        # Si hay dataset ampliado en memoria, usarlo
        if st.session_state["dataset_ampliado"] is not None:
            st.info("üì¶ Usando dataset ampliado almacenado en memoria.")
            df = st.session_state["dataset_ampliado"]
        else:
            st.warning("‚ö†Ô∏è No hay dataset ampliado en memoria. Se usar√° el dataset base.")
            df = preparar_datos_regresion(datasets)

        if not df.empty:
            comparar_regresiones(df)

    elif submenu == "Clasificaci√≥n (Matriz de Confusi√≥n)":
        if st.session_state["dataset_ampliado"] is not None:
            st.info("üì¶ Usando dataset ampliado almacenado en memoria.")
            df = st.session_state["dataset_ampliado"]
        else:
            st.warning("‚ö†Ô∏è No hay dataset ampliado en memoria. Se usar√° el dataset base.")
            df = preparar_datos_regresion(datasets)

        if not df.empty:
            mostrar_matriz_confusion(df)

